{
  "analyticsCode": "",
  "pageLayout": "two-column",
  "basics": {
    "name": "Masood Salman Choudhury",
    "label": "Senior Data Engineer & AI Solutions Architect",
    "image": "/DP_cropped.jpg",
    "email": "MSalman5230@gmail.com",
    "url": "https://msalman.de",
    "summary": "Senior Data Engineer & AI Solutions Architect with 5+ years' experience delivering end-to-end data platforms and intelligent applications across fintech, SaaS, and industrial analytics. Expert in designing scalable data pipelines, building AI-powered systems with LLMs and machine learning models, and deploying robust cloud-native solutions on AWS, Azure, and GCP.",
    "theme": "blue",
    "location": {
      "address": "Manchester",
      "city": "Manchester",
      "countryCode": "GB",
      "region": "United Kingdom"
    },
    "profiles": [
      {
        "network": "LinkedIn",
        "icon": "mdi:linkedin",
        "username": "MSalman5230",
        "url": "https://linkedin.com/in/MSalman5230"
      },
      {
        "network": "GitHub",
        "icon": "mdi:github",
        "username": "MSalman5230",
        "url": "https://github.com/MSalman5230"
      },
      {
        "network": "Website",
        "icon": "mdi:web",
        "username": "msalman.de",
        "url": "https://msalman.de"
      }
    ],
    "beian": {
      "mint": "",
      "police": ""
    }
  },

  "education": [
    {
      "institution": "University of Liverpool",
      "url": "",
      "area": "MSc Business Analytics and Big Data",
      "studyType": "Masters",
      "startDate": "2020-11-01",
      "endDate": "2022-02-01",
      "result": "Distinction",
      "location": "Liverpool, United Kingdom",
      "keyModules": [
        "Data Mining & Machine Learning",
        "Big Data Analytics",
        "Enterprise Systems with SAP",
        "Digital Business Technology and Management",
        "Digital Strategy"
      ]
    },
    {
      "institution": "Asian Institute of Management and Technology",
      "url": "",
      "area": "Bachelor of Business Administration",
      "studyType": "Bachelors",
      "startDate": "2016-08-01",
      "endDate": "2019-06-01",
      "location": "Guwahati, India",
      "keyModules": ["Statistics", "Mathematics", "Production & Operation Management"],
      "result": "First-Class"
    }
  ],
  "certificates": [
    {
      "name": "IBM Data Science Professional Certificate",
      "date": "",
      "url": "",
      "issuer": "IBM"
    },
    {
      "name": "Microsoft Certified: Azure Fundamentals (AZ-900)",
      "date": "",
      "url": "",
      "issuer": "Microsoft"
    },
    {
      "name": "Microsoft Certified: Azure AI Fundamentals (AI-900)",
      "date": "",
      "url": "",
      "issuer": "Microsoft"
    }
  ],
  "work": [
    {
      "name": "Helixiora",
      "position": "Senior Data Engineer & AI Solutions Architect",
      "location_type": "Remote",
      "location": "The Hague, Netherlands",
      "url": "",
      "startDate": "2024-04-01",
      "endDate": null,
      "summary": "Worked as a lead data engineer delivering end-to-end solutions for multiple international clients across fintech, enterprise SaaS, and industrial analytics domains. Responsible for architecting and implementing data platforms, AI solutions, and cloud-based applications.",

             "responsibilities": [
         "Designed, built, and deployed Azure data pipelines in Databricks (PySpark, SparkSQL) to ingest large-scale structured and semi-structured datasets from Blob Storage and Azure Cosmos DB, execute complex transformations, and persist curated features in Delta Lake for downstream machine learning workflows",
         "Led development of an enterprise-grade Multi-Stage RAG system for clients, combining Python-based data pipelines (Google Drive, Slack) with LLMs, Langchain, Reranker, and Pinecone VectorDB to deliver highly accurate, context-aware retrieval workflows",
         "Designed, built, and deployed an end-to-end Azure data pipeline using App Functions and CosmosDB, applying Kimball dimensional modeling for Power BI datasets with DAX-based measures, enabling KPI reporting and forecasting",
         "Led development of a full-stack AI SaaS app (Android and iOS) leveraging OpenAI Assistant API with a FastAPI backend, React Native (Expo) frontend, and PostgreSQL database, including OAuth 2.0 authentication and integrated Stripe payment processing",
         "Designed and implemented end-to-end CI/CD pipelines for fully automated deployment of containerized applications to AWS, ECS using Docker, Docker Compose, and infrastructure-as-code best practices (Terraform)",
         "Secured and deployed SaaS applications with SSL, reverse proxies, Zero Trust controls, and firewall rules, optimising load balancing for high availability, and set up logging and monitoring with Prometheus and Grafana to ensure system reliability and observability.",
         "Provided technical mentorship and conducted code reviews for junior and mid-level engineers, promoting best practices, improving code quality, and accelerating team growth"
       ],
             "skills": {
         "Python": "simple-icons:python",
         "Databricks": "simple-icons:databricks",
         "PySpark": "simple-icons:apachespark",
         "Delta Lake": "simple-icons:delta",
         "Langchain": "simple-icons:langchain",
         "Pinecone": "mdi:database-search",
         "React Native": "mdi:react",
         "PostgreSQL": "simple-icons:postgresql",
         "MySQL": "simple-icons:mysql",
         "Docker": "mdi:docker",
         "AWS": "simple-icons:amazonaws",
         "Azure": "simple-icons:microsoftazure",
         "Terraform": "simple-icons:terraform",
         "Git": "mdi:git",
         "Prometheus": "simple-icons:prometheus",
         "Grafana": "simple-icons:grafana",
         "SSL": "mdi:lock",
         "Nginx": "simple-icons:nginx"
       }
    },
    {
      "name": "Vaultoro",
      "position": "Senior Data Engineer",
      "location_type": "On-Site",
      "location": "London, United Kingdom",
      "url": "",
      "startDate": "2022-02-01",
      "endDate": "2024-03-01",
      "summary": "Architected complete data warehouse solutions and led development of scalable ETL pipelines for financial data processing.",

             "responsibilities": [
         "Architected a Kimball style star schema data warehouse using Elasticsearch and BigQuery, enabling real-time KPI dashboards in Kibana and empowering data-driven decision-making for stakeholders",
         "Led the design and implementation of multiple scalable ETL pipelines with Python, GCP Dataflow, Scrapy, and managed workflow orchestration with Apache Airflow, processing over 10 million financial data rows daily for real-time analytics",
         "Co-led the agile development of a secure, scalable Savings Platform using FastAPI, delivering the product in 3 months; currently manages $2M+ in monthly customer deposits",
         "Managed and optimized databases including MongoDB, PostgreSQL, Elasticsearch, and BigQuery by tuning queries, indexes, and partitions, resulting in significant performance improvements",
         "Conducted deep data analysis with Pandas to detect anomalies and identify potential fraud patterns, enhancing platform security",
         "Automated data validation workflows using Python scripts, ensuring pipeline integrity and achieving 100% uptime for critical microservices",
         "Deployed containerized applications with Docker and Kubernetes, ensuring high availability, scalability, and streamlined CI/CD operations",
         "Developed and implemented a Random Forest classification model to identify high-value clients during signup, enabling personalized onboarding experiences",
         "Configured and monitored Google Analytics and Tag Manager dashboards to track user behavior and support data-driven marketing strategies"
       ],
      "skills": {
        "Python": "simple-icons:python",
        "Elasticsearch": "simple-icons:elasticsearch",
        "BigQuery": "simple-icons:googlebigquery",
        "Kibana": "simple-icons:kibana",
        "GCP": "simple-icons:googlecloud",
        "FastAPI": "simple-icons:fastapi",
        "MongoDB": "simple-icons:mongodb",
        "PostgreSQL": "simple-icons:postgresql",
        "Apache Airflow": "simple-icons:apacheairflow",
        "Pandas": "simple-icons:pandas",
        "Docker": "mdi:docker",
        "Kubernetes": "simple-icons:kubernetes"
      }
    },
    {
      "name": "SoftCrop IT",
      "position": "Data Analyst",
      "location_type": "On-site",
      "location": "Guwahati, India",
      "url": "",
      "startDate": "2019-06-01",
      "endDate": "2020-10-01",
      "summary": "Delivered actionable insights via Tableau dashboards and automated data collection processes.",

             "responsibilities": [
         "Delivered actionable insights via Tableau dashboards (waterfall/cohort analysis), improving stakeholder decision-making",
         "Automated competitor data scraping (Scrapy) and ETL into MySQL, reducing manual effort by 50%",
         "Analyzed sales and geographic data to guide strategic fibre network expansion"
       ],
      "skills": {
        "Python": "simple-icons:python",
        "Tableau": "simple-icons:tableau",
        "MySQL": "simple-icons:mysql",
        "Scrapy": "simple-icons:scrapy",
        "Pandas": "simple-icons:pandas"
      }
    }
  ],
  "skills": [
    {
      "name": "Python",
      "icon": "simple-icons:python",
      "level": "Expert",
      "keywords": [
        "Data Engineering",
        "Machine Learning",
        "Backend Development",
        "Automation"
      ]
    },
    {
      "name": "Databricks",
      "icon": "simple-icons:databricks",
      "level": "Expert",
      "keywords": [
        "Data Processing",
        "Big Data",
        "PySpark",
        "Delta Lake"
      ]
    },
    {
      "name": "PySpark",
      "icon": "simple-icons:apachespark",
      "level": "Expert",
      "keywords": [
        "Big Data Processing",
        "Distributed Computing",
        "Data Transformation",
        "ETL Pipelines"
      ]
    },
    {
      "name": "Delta Lake",
      "icon": "simple-icons:delta",
      "level": "Expert",
      "keywords": [
        "Data Lake",
        "ACID Transactions",
        "Data Versioning",
        "Big Data Storage"
      ]
    },
    {
      "name": "Langchain",
      "icon": "simple-icons:langchain",
      "level": "Expert",
      "keywords": [
        "LLM Integration",
        "RAG Systems",
        "AI Applications",
        "Prompt Engineering"
      ]
    },
    {
      "name": "Pinecone",
      "icon": "mdi:database-search",
      "level": "Expert",
      "keywords": [
        "Vector Database",
        "Semantic Search",
        "AI Applications",
        "RAG Systems"
      ]
    },
    {
      "name": "React Native",
      "icon": "mdi:react",
      "level": "Intermediate",
      "keywords": [
        "Mobile Development",
        "Cross-platform",
        "JavaScript",
        "UI Development"
      ]
    },
    {
      "name": "PostgreSQL",
      "icon": "simple-icons:postgresql",
      "level": "Expert",
      "keywords": [
        "Database Management",
        "SQL",
        "Data Storage",
        "Backend"
      ]
    },
    {
      "name": "MySQL",
      "icon": "simple-icons:mysql",
      "level": "Intermediate",
      "keywords": [
        "Database Management",
        "SQL",
        "Data Storage",
        "Backend"
      ]
    },
    {
      "name": "Docker",
      "icon": "mdi:docker",
      "level": "Expert",
      "keywords": [
        "Containerization",
        "DevOps",
        "Deployment",
        "Microservices"
      ]
    },
    {
      "name": "AWS",
      "icon": "simple-icons:amazonaws",
      "level": "Expert",
      "keywords": [
        "Cloud Computing",
        "ECS",
        "Lambda",
        "Infrastructure"
      ]
    },
    {
      "name": "Azure",
      "icon": "simple-icons:microsoftazure",
      "level": "Expert",
      "keywords": [
        "Cloud Computing",
        "Functions",
        "CosmosDB",
        "Data Platform"
      ]
    },
    {
      "name": "GCP",
      "icon": "simple-icons:googlecloud",
      "level": "Intermediate",
      "keywords": [
        "Cloud Computing",
        "BigQuery",
        "Dataflow",
        "Pub/Sub"
      ]
    },
    {
      "name": "Terraform",
      "icon": "simple-icons:terraform",
      "level": "Intermediate",
      "keywords": [
        "Infrastructure as Code",
        "DevOps",
        "Cloud Provisioning",
        "Automation"
      ]
    },
    {
      "name": "Git",
      "icon": "mdi:git",
      "level": "Expert",
      "keywords": [
        "Version Control",
        "Collaboration",
        "Source Code Management",
        "CI/CD"
      ]
    },
    {
      "name": "Elasticsearch",
      "icon": "simple-icons:elasticsearch",
      "level": "Intermediate",
      "keywords": [
        "Search Engine",
        "Data Indexing",
        "Analytics",
        "Big Data"
      ]
    },
    {
      "name": "BigQuery",
      "icon": "simple-icons:googlebigquery",
      "level": "Intermediate",
      "keywords": [
        "Data Warehouse",
        "Big Data Analytics",
        "SQL",
        "Cloud Storage"
      ]
    },
    {
      "name": "FastAPI",
      "icon": "simple-icons:fastapi",
      "level": "Expert",
      "keywords": [
        "Web Framework",
        "API Development",
        "Python",
        "Backend"
      ]
    },
    {
      "name": "MongoDB",
      "icon": "simple-icons:mongodb",
      "level": "Intermediate",
      "keywords": [
        "NoSQL Database",
        "Document Storage",
        "Data Management",
        "Backend"
      ]
    },
    {
      "name": "Apache Airflow",
      "icon": "simple-icons:apacheairflow",
      "level": "Intermediate",
      "keywords": [
        "Workflow Orchestration",
        "ETL Pipelines",
        "Data Engineering",
        "Automation"
      ]
    },
    {
      "name": "Pandas",
      "icon": "simple-icons:pandas",
      "level": "Expert",
      "keywords": [
        "Data Analysis",
        "Data Manipulation",
        "Python",
        "Analytics"
      ]
    },
    {
      "name": "Kubernetes",
      "icon": "simple-icons:kubernetes",
      "level": "Intermediate",
      "keywords": [
        "Container Orchestration",
        "DevOps",
        "Scalability",
        "Microservices"
      ]
    },
    {
      "name": "Tableau",
      "icon": "simple-icons:tableau",
      "level": "Intermediate",
      "keywords": [
        "Data Visualization",
        "Business Intelligence",
        "Analytics",
        "Reporting"
      ]
    },
         {
       "name": "Scrapy",
       "icon": "simple-icons:scrapy",
       "level": "Intermediate",
       "keywords": [
         "Web Scraping",
         "Data Collection",
         "Python",
         "Automation"
       ]
     },
     {
       "name": "Prometheus",
       "icon": "simple-icons:prometheus",
       "level": "Intermediate",
       "keywords": [
         "Monitoring",
         "Metrics",
         "Observability",
         "DevOps"
       ]
     },
     {
       "name": "Grafana",
       "icon": "simple-icons:grafana",
       "level": "Intermediate",
       "keywords": [
         "Data Visualization",
         "Monitoring",
         "Dashboards",
         "Observability"
       ]
     },
     {
       "name": "Nginx",
       "icon": "simple-icons:nginx",
       "level": "Intermediate",
       "keywords": [
         "Web Server",
         "Reverse Proxy",
         "Load Balancing",
         "High Availability"
       ]
     }
  ],
  "languages": [
    {
      "language": "English",
      "fluency": "Fluent"
    },
    {
      "language": "Bengali",
      "fluency": "Native speaker"
    }
  ],
  "interests": [
    {
      "name": "Data Science",
      "keywords": [
        "Machine Learning",
        "AI",
        "Big Data Analytics"
      ]
    },
    {
      "name": "Technology",
      "keywords": [
        "Cloud Computing",
        "DevOps",
        "Software Architecture"
      ]
    }
  ],
  "references": [
    {
      "name": "Available upon request",
      "reference": "Professional references available upon request"
    }
  ],
  "projects": [
    {
      "name": "Homelab Docker Compose Stack",
      "isActive": true,
      "description": "A comprehensive collection of production-ready Docker Compose stacks for self-hosted services, including monitoring, databases, home automation, and infrastructure management tools for personal homelab environment.",
      "highlights": [
        "üê≥ Designed and deployed 15+ production-ready Docker Compose stacks for self-hosted services",
        "üìä Implemented comprehensive monitoring stack with Prometheus, Grafana, cAdvisor, and Node Exporter for infrastructure visibility",
        "üîç Built Elasticsearch stack for log aggregation and search capabilities with Kibana visualization",
        "üè† Integrated Home Assistant for smart home automation and IoT device management",
        "üíæ Configured MySQL and database services with optimized Docker configurations for data persistence",
        "üöÄ Deployed Portainer for container management and orchestration with streamlined deployment workflows",
        "‚ö° Implemented performance testing tools (LibreSpeed, OpenSpeedTest) for network and system benchmarking"
      ],
      "url": "https://github.com/MSalman5230/My-Docker-Compose-Stack",
      "github": "https://github.com/MSalman5230/My-Docker-Compose-Stack"
    },
    {
      "name": "Anime Filler Tagger",
      "isActive": true,
      "description": "A Python automation script that automatically tags anime episodes as filler or canon by scraping animefillerlist.com and intelligently renaming files with metadata for better organization and viewing experience.",
      "highlights": [
        "üîç Built automated web scraper to extract episode metadata from animefillerlist.com for accurate filler/canon classification",
        "üìÅ Implemented intelligent file renaming system that preserves quality tags while adding filler/canon metadata",
        "‚öôÔ∏è Developed configurable system supporting multiple anime series with customizable quality tags and file paths",
        "üéØ Created smart episode detection algorithm to handle various filename formats and episode numbering schemes",
        "üìä Automated metadata integration that enhances media library organization and viewing experience",
        "üöÄ Streamlined workflow for anime enthusiasts to efficiently manage large episode collections"
      ],
      "url": "https://github.com/MSalman5230/Anime_Filler_Tagger",
      "github": "https://github.com/MSalman5230/Anime_Filler_Tagger"
    },
    {
      "name": "Game Hand Gesture Detection",
      "isActive": true,
      "description": "A computer vision-based hand gesture recognition system using TensorFlow and OpenCV for real-time gaming control, enabling hands-free game interaction through custom-trained machine learning models.",
      "highlights": [
        "ü§ñ Built custom hand gesture detection model using TensorFlow transfer learning with SSD MobileNet V2 FPNLite architecture",
        "üì∏ Developed automated image collection pipeline for custom hand gesture dataset creation and labeling",
        "üéØ Implemented real-time hand gesture recognition from video feed for gaming applications",
        "üéÆ Created practical gaming integration demonstrated with Chrome Dino game control",
        "üî¨ Applied transfer learning techniques to optimize model performance for specific gesture recognition tasks",
        "üìä Structured project workflow with Jupyter notebooks for data collection, training, and detection phases"
      ],
      "url": "https://github.com/MSalman5230/Game_Hand_Gesture",
      "github": "https://github.com/MSalman5230/Game_Hand_Gesture"
    },
    {
      "name": "Nvidia DLSS DLL Updater",
      "isActive": true,
      "description": "A Python-based automation tool that downloads and applies the latest Nvidia DLSS and DLSS Frame Generation DLLs to local games, ensuring optimal gaming performance and visual quality.",
      "highlights": [
        "üîß Built automated DLL management system for Nvidia DLSS and Frame Generation updates",
        "üåê Integrated with TechPowerUp.com API for real-time DLL version monitoring and downloads",
        "üìÅ Implemented intelligent file discovery to locate DLSS DLLs across multiple game directories",
        "üíæ Created automated backup system with timestamped naming for safe DLL rollback",
        "‚öôÔ∏è Developed configurable system supporting multiple game library paths and server locations",
        "üöÄ Packaged as executable with PyInstaller for easy distribution and deployment",
        "‚≠ê Achieved 10+ stars on GitHub demonstrating community recognition and utility"
      ],
      "url": "https://github.com/MSalman5230/Nvidia_DLSS_DLL_Updater",
      "github": "https://github.com/MSalman5230/Nvidia_DLSS_DLL_Updater"
    },
    {
      "name": "Gameloot Stock Alert Bot",
      "isActive": true,
      "description": "A Python-based web scraping application that monitors Gameloot.in for PC component stock changes and sends real-time Telegram notifications when products become available or go out of stock.",
      "highlights": [
        "üï∑Ô∏è Built automated web scraper using BeautifulSoup and Python for real-time PC component monitoring",
        "üìä Implemented MongoDB integration for persistent storage and smart deduplication of products",
        "ü§ñ Developed Telegram bot integration for instant stock change notifications",
        "üîç Implemented change detection algorithms to identify new products, restocked items, and sold-out products",
        "üìù Comprehensive logging system with configurable levels for monitoring and debugging",
        "üöÄ Automated deployment with error handling and retry mechanisms for robust operation"
      ],
      "url": "https://github.com/MSalman5230/Gameloot-scrape-alert",
      "github": "https://github.com/MSalman5230/Gameloot-scrape-alert"
    },
    {
      "name": "ETA Prediction for Container Ports using Machine Learning",
      "isActive": false,
      "description": "MSc project at University of Liverpool predicting ship delays with 80% accuracy using machine learning techniques.",
      "highlights": [
        "üéØ Predicted ship delays 10+ days in advance with 80% accuracy",
        "üîß Cleaned and engineered features using Pandas, NumPy, and Pearson correlation",
        "ü§ñ Evaluated multiple ML models (SVM, DT, RF, NN) and selected Random Forest",
        "üöÄ Deployed the model with FastAPI for real-time predictions"
      ],
      "url": "",
      "github": ""
    },
    {
      "name": "Diabetes Prediction using XGBoost",
      "isActive": true,
      "description": "Personal project for self-learning diabetes prediction using machine learning techniques.",
      "highlights": [
        "üìä Exploratory Data Analysis (EDA) with Seaborn and UMAP visualization",
        "‚öôÔ∏è Tuned XGBoost hyperparameters with Optuna and ML-Flow",
        "üåê Deployed using Streamlit framework"
      ],
      "url": "https://github.com/MSalman5230/Diabetes-Prediction",
      "github": "https://github.com/MSalman5230/Diabetes-Prediction"
    }
  ]
}
